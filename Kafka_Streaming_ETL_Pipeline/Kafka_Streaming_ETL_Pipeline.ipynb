{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Real-Time Twitter Data Pipeline with Kafka, PostgreSQL, and PySpark\n",
                "\n",
                "## Project Overview\n",
                "This project demonstrates a real-time ETL (Extract, Transform, Load) pipeline that:\n",
                "1.  **Extracts** tweets from the Twitter API using `tweepy`.\n",
                "2.  **Streams** the data to a **Kafka** topic (`tweet-crawl`) using a Python producer.\n",
                "3.  **Consumes** the data from Kafka and **Loads** it into a **PostgreSQL** database (`tweet_merge` table).\n",
                "4.  **Transforms** the stored data using **PySpark** (e.g., text normalization) and writes the results back to PostgreSQL (`tweet_transformed` table).\n",
                "\n",
                "## Tech Stack\n",
                "- **Source**: Twitter API v2\n",
                "- **Streaming**: Apache Kafka\n",
                "- **Storage**: PostgreSQL\n",
                "- **Processing**: PySpark\n",
                "- **Language**: Python 3.12\n",
                "\n",
                "## Prerequisites\n",
                "- Running Kafka Broker (e.g., via Docker)\n",
                "- Running PostgreSQL Instance\n",
                "- Twitter Developer Account (Bearer Token)\n",
                "\n",
                "---\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install required packages (run once)\n",
                "!pip install -q tweepy kafka-python psycopg2-binary pyspark\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Configuration\n",
                "Load sensitive credentials and configuration from environment variables for security.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import json\n",
                "import time\n",
                "\n",
                "# Twitter API\n",
                "BEARER_TOKEN = os.getenv(\"TWITTER_BEARER_TOKEN\", \"<YOUR_BEARER_TOKEN>\")\n",
                "\n",
                "# Kafka\n",
                "KAFKA_BOOTSTRAP_SERVERS = os.getenv(\"KAFKA_BOOTSTRAP_SERVERS\", \"localhost:9092\")\n",
                "KAFKA_TOPIC = \"tweet-crawl\"\n",
                "\n",
                "# PostgreSQL\n",
                "DB_HOST = os.getenv(\"DB_HOST\", \"localhost\")\n",
                "DB_PORT = os.getenv(\"DB_PORT\", \"5432\")\n",
                "DB_NAME = os.getenv(\"DB_NAME\", \"twitter_db\")\n",
                "DB_USER = os.getenv(\"DB_USER\", \"admin\")\n",
                "DB_PASS = os.getenv(\"DB_PASS\", \"admin123\")\n",
                "\n",
                "print(\"Configuration loaded.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 1: Kafka Producer (Twitter -> Kafka)\n",
                "Fetches recent tweets about \"AI\" and publishes them to the Kafka topic.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import tweepy\n",
                "from kafka import KafkaProducer\n",
                "\n",
                "def run_producer():\n",
                "    if BEARER_TOKEN == \"<YOUR_BEARER_TOKEN>\":\n",
                "        print(\"Please set your Twitter Bearer Token.\")\n",
                "        return\n",
                "\n",
                "    try:\n",
                "        # Initialize Twitter Client\n",
                "        client = tweepy.Client(bearer_token=BEARER_TOKEN)\n",
                "        \n",
                "        # Initialize Kafka Producer\n",
                "        producer = KafkaProducer(\n",
                "            bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n",
                "            value_serializer=lambda v: json.dumps(v).encode(\"utf-8\")\n",
                "        )\n",
                "        \n",
                "        query = \"AI -is:retweet lang:en\"\n",
                "        response = client.search_recent_tweets(query=query, tweet_fields=[\"id\", \"text\", \"created_at\"], max_results=10)\n",
                "        \n",
                "        if response.data:\n",
                "            for tweet in response.data:\n",
                "                tweet_data = {\n",
                "                    \"id\": tweet.id,\n",
                "                    \"text\": tweet.text,\n",
                "                    \"created_at\": str(tweet.created_at)\n",
                "                }\n",
                "                producer.send(KAFKA_TOPIC, tweet_data)\n",
                "                print(f\"Produced -> ID: {tweet.id}\")\n",
                "        \n",
                "        producer.flush()\n",
                "        print(\"Producer finished.\")\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(f\"Producer Error: {e}\")\n",
                "\n",
                "# Uncomment to run the producer\n",
                "# run_producer()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 2: Kafka Consumer (Kafka -> PostgreSQL)\n",
                "Consumes messages from Kafka and inserts them into the `tweet_merge` table.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import psycopg2\n",
                "from kafka import KafkaConsumer\n",
                "\n",
                "def run_consumer():\n",
                "    try:\n",
                "        # Connect to PostgreSQL\n",
                "        conn = psycopg2.connect(\n",
                "            dbname=DB_NAME,\n",
                "            user=DB_USER,\n",
                "            password=DB_PASS,\n",
                "            host=DB_HOST,\n",
                "            port=DB_PORT\n",
                "        )\n",
                "        cursor = conn.cursor()\n",
                "        \n",
                "        # Create table\n",
                "        cursor.execute(\"\"\"\n",
                "            CREATE TABLE IF NOT EXISTS tweet_merge (\n",
                "                id BIGINT PRIMARY KEY,\n",
                "                text TEXT,\n",
                "                created_at TIMESTAMP\n",
                "            );\n",
                "        \"\"\")\n",
                "        conn.commit()\n",
                "        \n",
                "        # Initialize Consumer\n",
                "        consumer = KafkaConsumer(\n",
                "            KAFKA_TOPIC,\n",
                "            bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n",
                "            auto_offset_reset=\"earliest\",\n",
                "            enable_auto_commit=True,\n",
                "            group_id=\"tweet-consumer-group\",\n",
                "            value_deserializer=lambda v: json.loads(v.decode(\"utf-8\"))\n",
                "        )\n",
                "        \n",
                "        print(\"Consumer started. Listening for messages...\")\n",
                "        # Note: In a real app, this would run indefinitely. Here we consume a few for demo.\n",
                "        for i, message in enumerate(consumer):\n",
                "            tweet = message.value\n",
                "            try:\n",
                "                cursor.execute(\n",
                "                    \"INSERT INTO tweet_merge (id, text, created_at) VALUES (%s, %s, %s) ON CONFLICT (id) DO NOTHING;\",\n",
                "                    (tweet[\"id\"], tweet[\"text\"], tweet.get(\"created_at\"))\n",
                "                )\n",
                "                conn.commit()\n",
                "                print(f\"Inserted -> {tweet['id']}\")\n",
                "            except Exception as e:\n",
                "                print(f\"DB Error: {e}\")\n",
                "                conn.rollback()\n",
                "            \n",
                "            if i >= 9: # Stop after 10 messages for demo\n",
                "                break\n",
                "                \n",
                "        cursor.close()\n",
                "        conn.close()\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(f\"Consumer Error: {e}\")\n",
                "\n",
                "# Uncomment to run the consumer\n",
                "# run_consumer()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 3: PySpark Transformation (PostgreSQL -> PySpark -> PostgreSQL)\n",
                "Reads from `tweet_merge`, converts text to uppercase, and writes to `tweet_transformed`.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pyspark.sql import SparkSession\n",
                "from pyspark.sql.functions import col, upper\n",
                "\n",
                "def run_spark_job():\n",
                "    try:\n",
                "        # Initialize Spark Session with PostgreSQL Driver\n",
                "        # Note: Ensure the PostgreSQL JDBC driver jar is available to Spark\n",
                "        spark = SparkSession.builder \\\n",
                "            .appName(\"TweetTransformation\") \\\n",
                "            .config(\"spark.jars\", \"postgresql-42.2.18.jar\") \\\n",
                "            .getOrCreate()\n",
                "        \n",
                "        jdbc_url = f\"jdbc:postgresql://{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
                "        db_props = {\n",
                "            \"user\": DB_USER,\n",
                "            \"password\": DB_PASS,\n",
                "            \"driver\": \"org.postgresql.Driver\"\n",
                "        }\n",
                "        \n",
                "        # Read Data\n",
                "        tweets_df = spark.read.jdbc(url=jdbc_url, table=\"tweet_merge\", properties=db_props)\n",
                "        \n",
                "        # Transform Data\n",
                "        transformed_df = tweets_df.withColumn(\"text_uppercase\", upper(col(\"text\")))\n",
                "        \n",
                "        print(\"--- Transformed Data Preview ---\")\n",
                "        transformed_df.select(\"id\", \"text_uppercase\").show(5, truncate=False)\n",
                "        \n",
                "        # Write Data Back\n",
                "        transformed_df.write.jdbc(\n",
                "            url=jdbc_url,\n",
                "            table=\"tweet_transformed\",\n",
                "            mode=\"overwrite\",\n",
                "            properties=db_props\n",
                "        )\n",
                "        print(\"Transformation job completed successfully.\")\n",
                "        \n",
                "        spark.stop()\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(f\"Spark Error: {e}. Ensure PostgreSQL JDBC driver is in your path.\")\n",
                "\n",
                "# Uncomment to run the Spark job\n",
                "# run_spark_job()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}