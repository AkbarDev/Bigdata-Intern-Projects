{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f120ea98-9026-463b-b3cb-a9b08f321988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STEP 1: Import libraries --- #\n",
    "\n",
    "import requests, pandas,pytorch,tensorflow\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d5ca10-8d8b-47ea-87b5-fa3b6338485b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STEP 2: Initialize Hugging Face models —#\n",
    "\n",
    "sentiment_model = pipeline(\"sentiment-analysis\",model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\",revision=\"714eb0f\")\n",
    "\n",
    "#---Notes:---#\n",
    "\n",
    "This line creates a sentiment analysis model using HuggingFace’s pipeline API.\n",
    "\n",
    "It loads a fine-tuned DistilBERT model specifically trained to classify text into: [This is the popular SST-2 (Stanford Sentiment Treebank v2) dataset]\n",
    "\t•\tPOSITIVE\n",
    "\t•\tNEGATIVE\n",
    "\n",
    "(a) pipeline(\"sentiment-analysis\") ----->This tells HuggingFace to Load a model that can analyze the sentiment of text.The pipeline abstracts all the complexity such as:\n",
    "\t•\tTokenization\n",
    "\t•\tModel inference\n",
    "\t•\tDecoding\n",
    "\n",
    "(b) model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\",This selects the specific model you want to use.\n",
    "  \n",
    "\t•\tDistilBERT → A smaller, faster version of BERT\n",
    "\t•\tbase-uncased → Converts text to lowercase internally\n",
    "\t•\tfine-tuned on SST-2 → Model was trained to determine positive or negative tone in English sentences\n",
    "\n",
    "(c)revision=\"714eb0f\" ------> This chooses a specific version/commit of the model from HuggingFace .This is important because,\n",
    "\n",
    "✔ Ensures version consistency\n",
    "✔ Avoids changes in model weights in the future\n",
    "✔ Makes your project reproducible\n",
    "\n",
    "If HuggingFace updates the model later, your code still uses the exact same model snapshot.\n",
    "\n",
    "  Example:\n",
    "\n",
    "1. sentiment_model(\"The product quality is excellent!\")\n",
    "\n",
    "Ouput : [{\"label\": \"POSITIVE\", \"score\": 0.998}]\n",
    "\n",
    "2. sentiment_model(\"The flight was delayed and the service was terrible.\")]\n",
    "\n",
    "output :  [{\"label\": \"NEGATIVE\", \"score\": 0.996}] \n",
    "\n",
    "#-Notes-#\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e930d59-4a6e-418b-9d25-f1057b2be14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---- Step 3: # Summarization (Use BART for summarization ---#)\n",
    "\n",
    "summarizer_model = pipeline(\"summarization\",model=\"facebook/bart-large-cnn”)\n",
    "\n",
    "#--Notes--#\n",
    "                            \n",
    "This line creates a text-summarization model using the HuggingFace pipeline function.\n",
    "\n",
    "Key components:\n",
    "\n",
    "(A) pipeline(“summarization”)\n",
    "Tells HuggingFace to load a model that can summarize long text into a shorter version.\n",
    "\n",
    "(B) model=“facebook/bart-large-cnn”\n",
    "Loads the BART-Large-CNN model from Facebook AI — one of the best models for abstract summarization.\n",
    "\n",
    "(C) What does BART do?\n",
    "\n",
    "\t•\tReads long paragraphs of news\n",
    "\t•\tUnderstands context\n",
    "\t•\tGenerates human-like summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83aa521-1c6f-4866-82e5-058fe3d5b7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---- step 4: Named Entity Recognition model (BERT fine-tuned for NER) -------- #\n",
    "\n",
    "ner_model = pipeline(\"ner\",model=\"dslim/bert-base-NER\",grouped_entities=True)\n",
    "\n",
    "#---Notes---#:\n",
    "\n",
    "This line creates a model that can detect entities in text.\n",
    "\n",
    "Key components:\n",
    "\n",
    "(a) pipeline(“ner”)\n",
    "Loads a model for Named Entity Recognition.\n",
    "\n",
    "(b)model=“dslim/bert-base-NER”\n",
    "Uses a fine-tuned BERT model that detects:\n",
    "•\tPERSON\n",
    "\t•\tLOCATION\n",
    "\t•\tORGANIZATION\n",
    "\t•\tDATE\n",
    "\t•\tPRODUCT\n",
    "\t•\tEVENT\n",
    "\n",
    "(c)grouped_entities=True\n",
    "\n",
    "Groups multi-word entities together.\n",
    "\n",
    "Example:\n",
    "\"New\", \"York\" are separate\n",
    "With this:\n",
    "\"New York\" becomes one entity\n",
    "\n",
    "(d) What does NER do?\n",
    "\n",
    "It extracts structured information:\n",
    "\t•\tPeople mentioned\n",
    "\t•\tCompanies\n",
    "\t•\tPlaces\n",
    "\t•\tDates\n",
    "\t•\tKey topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e98f7ff-3aae-4ec9-b154-3205203bf2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----- Step 5: Fetch Live News ------------#\n",
    "\n",
    "API_KEY = “Enter your News API key”\n",
    "\n",
    "query = \"artificial intelligence\" [Enter custom keyword to fetch the content news]\n",
    "\n",
    "url = f\"https://newsapi.org/v2/everything?q={query}&language=en&pageSize=5&apiKey={API_KEY}\"  \n",
    "\n",
    "#--- Notes: [https://newsapi.org/v2/everything (News API Endpoint),q={query} Search keyword,language=en \n",
    "#Return only English articles,pageSize=5 Limit results to 5 articles,apiKey={API_KEY},Your secret API key (required for authentication) ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d7f702-5809-40b4-bf96-89becba7bb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------- STEP 6: Calls News API (HTTP GET Request0  & JSON data conversion to python dictionary ------------- #\n",
    "\n",
    "\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "\n",
    "(a) response = requests.get(url)\n",
    "\n",
    "This line sends an HTTP GET request to the API (NewsAPI) using the URL built earlier in step 5:\n",
    "\n",
    "The result is stored in response, which is a Response object containing:\n",
    "\n",
    "\t•\tstatus code (200, 404, 500, etc.)\n",
    "\t•\theaders\n",
    "\t•\tthe actual data sent by the API (in JSON format)\n",
    "\n",
    "(b) data = response.json()\n",
    "\n",
    "This takes the raw JSON response and converts it into a Python dictionary.\n",
    "\n",
    "NewsAPI sends data like:\n",
    "\n",
    "{\n",
    "  \"status\": \"ok\",\n",
    "  \"totalResults\": 5,\n",
    "  \"articles\": [\n",
    "    {...},\n",
    "    {...}\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ed0a98-230d-4121-a92e-cb3c48909901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- Step 7 :----------------------#\n",
    "\n",
    "articles = data.get(\"articles\", [])\n",
    "df = pd.DataFrame(articles)[[\"title\", \"description\", \"url\"]]\n",
    "df.head()\n",
    "\n",
    "\n",
    "1.articles = data.get(\"articles\", [])\n",
    "\n",
    "data is the dictionary you received from the NewsAPI response.\n",
    "\n",
    ".get(\"articles\", []) tries to extract the value under the key \"articles\"\n",
    "\n",
    " -> If \"articles\" exists → it returns the list of articles.\n",
    " -> If \"articles\" doesn’t exist → it returns an empty list [] instead of crashing. \n",
    "\n",
    "\n",
    "2.df = pd.DataFrame(articles)[[\"title\", \"description\", \"url\"]]\n",
    "\n",
    "Converts the articles list into a pandas DataFrame.\n",
    "Then selects only three columns:\n",
    "\t•\t\"title\"\n",
    "\t•\t\"description\"\n",
    "\t•\t\"url\"\n",
    "\n",
    "3.df.head()\n",
    "Shows the first 5 rows of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b304bfc1-a35c-4770-8ab0-3b14f628e21e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858b6cbb-6c63-402b-a27d-0b8d1a72df6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STEP 1: Import libraries --- #\n",
    "\n",
    "import requests, pandas,pytorch,tensorflow\n",
    "!pip install transformers\n",
    "\n",
    "# --- STEP 2: Initialize Hugging Face models —#\n",
    "\n",
    "sentiment_model = pipeline(\"sentiment-analysis\",model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\",revision=\"714eb0f\")\n",
    "\n",
    "#---Notes:---#\n",
    "\n",
    "This line creates a sentiment analysis model using HuggingFace’s pipeline API.\n",
    "\n",
    "It loads a fine-tuned DistilBERT model specifically trained to classify text into: [This is the popular SST-2 (Stanford Sentiment Treebank v2) dataset]\n",
    "\t•\tPOSITIVE\n",
    "\t•\tNEGATIVE\n",
    "\n",
    "(a) pipeline(\"sentiment-analysis\") ----->This tells HuggingFace to Load a model that can analyze the sentiment of text.The pipeline abstracts all the complexity such as:\n",
    "\t•\tTokenization\n",
    "\t•\tModel inference\n",
    "\t•\tDecoding\n",
    "\n",
    "(b) model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\",This selects the specific model you want to use.\n",
    "  \n",
    "\t•\tDistilBERT → A smaller, faster version of BERT\n",
    "\t•\tbase-uncased → Converts text to lowercase internally\n",
    "\t•\tfine-tuned on SST-2 → Model was trained to determine positive or negative tone in English sentences\n",
    "\n",
    "(c)revision=\"714eb0f\" ------> This chooses a specific version/commit of the model from HuggingFace .This is important because,\n",
    "\n",
    "✔ Ensures version consistency\n",
    "✔ Avoids changes in model weights in the future\n",
    "✔ Makes your project reproducible\n",
    "\n",
    "If HuggingFace updates the model later, your code still uses the exact same model snapshot.\n",
    "\n",
    "  Example:\n",
    "\n",
    "1. sentiment_model(\"The product quality is excellent!\")\n",
    "\n",
    "Ouput : [{\"label\": \"POSITIVE\", \"score\": 0.998}]\n",
    "\n",
    "2. sentiment_model(\"The flight was delayed and the service was terrible.\")]\n",
    "\n",
    "output :  [{\"label\": \"NEGATIVE\", \"score\": 0.996}]\n",
    "\n",
    "\n",
    "#---- Step 3: # Summarization (Use BART for summarization ---#)\n",
    "\n",
    "summarizer_model = pipeline(\"summarization\",model=\"facebook/bart-large-cnn”)\n",
    "\n",
    "#--Notes--#\n",
    "                            \n",
    "This line creates a text-summarization model using the HuggingFace pipeline function.\n",
    "\n",
    "Key components:\n",
    "\n",
    "(A) pipeline(“summarization”)\n",
    "Tells HuggingFace to load a model that can summarize long text into a shorter version.\n",
    "\n",
    "(B) model=“facebook/bart-large-cnn”\n",
    "Loads the BART-Large-CNN model from Facebook AI — one of the best models for abstract summarization.\n",
    "\n",
    "(C) What does BART do?\n",
    "\n",
    "\t•\tReads long paragraphs of news\n",
    "\t•\tUnderstands context\n",
    "\t•\tGenerates human-like summaries\n",
    "\n",
    "\n",
    "#---- step 4: Named Entity Recognition model (BERT fine-tuned for NER) -------- #\n",
    "\n",
    "ner_model = pipeline(\"ner\",model=\"dslim/bert-base-NER\",grouped_entities=True)\n",
    "\n",
    "#---Notes---#:\n",
    "\n",
    "This line creates a model that can detect entities in text.\n",
    "\n",
    "Key components:\n",
    "\n",
    "(a) pipeline(“ner”)\n",
    "Loads a model for Named Entity Recognition.\n",
    "\n",
    "(b)model=“dslim/bert-base-NER”\n",
    "Uses a fine-tuned BERT model that detects:\n",
    "•\tPERSON\n",
    "\t•\tLOCATION\n",
    "\t•\tORGANIZATION\n",
    "\t•\tDATE\n",
    "\t•\tPRODUCT\n",
    "\t•\tEVENT\n",
    "\n",
    "(c)grouped_entities=True\n",
    "\n",
    "Groups multi-word entities together.\n",
    "\n",
    "Example:\n",
    "\"New\", \"York\" are separate\n",
    "With this:\n",
    "\"New York\" becomes one entity\n",
    "\n",
    "(d) What does NER do?\n",
    "\n",
    "It extracts structured information:\n",
    "\t•\tPeople mentioned\n",
    "\t•\tCompanies\n",
    "\t•\tPlaces\n",
    "\t•\tDates\n",
    "\t•\tKey topics\n",
    "\n",
    "\n",
    "\n",
    "#----- Step 5: Fetch Live News ------------#\n",
    "\n",
    "API_KEY = “Enter your News API key”\n",
    "\n",
    "query = \"artificial intelligence\" [Enter custom keyword to fetch the content news]\n",
    "\n",
    "url = f\"https://newsapi.org/v2/everything?q={query}&language=en&pageSize=5&apiKey={API_KEY}\"  \n",
    "\n",
    "#--- Notes: [https://newsapi.org/v2/everything (News API Endpoint),q={query} Search keyword,language=en Return only English articles,pageSize=5 Limit results to 5 articles,apiKey={API_KEY},Your secret API key (required for authentication) ]\n",
    "\n",
    "\n",
    "#---------- STEP 6: Calls News API (HTTP GET Request0  & JSON data conversion to python dictionary ------------- #\n",
    "\n",
    "\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "\n",
    "(a) response = requests.get(url)\n",
    "\n",
    "This line sends an HTTP GET request to the API (NewsAPI) using the URL built earlier in step 5:\n",
    "\n",
    "The result is stored in response, which is a Response object containing:\n",
    "\n",
    "\t•\tstatus code (200, 404, 500, etc.)\n",
    "\t•\theaders\n",
    "\t•\tthe actual data sent by the API (in JSON format)\n",
    "\n",
    "(b) data = response.json()\n",
    "\n",
    "This takes the raw JSON response and converts it into a Python dictionary.\n",
    "\n",
    "NewsAPI sends data like:\n",
    "\n",
    "{\n",
    "  \"status\": \"ok\",\n",
    "  \"totalResults\": 5,\n",
    "  \"articles\": [\n",
    "    {...},\n",
    "    {...}\n",
    "  ]\n",
    "}\n",
    "\n",
    "# ----------------- Step 7 :----------------------#\n",
    "\n",
    "articles = data.get(\"articles\", [])\n",
    "df = pd.DataFrame(articles)[[\"title\", \"description\", \"url\"]]\n",
    "df.head()\n",
    "\n",
    "\n",
    "1.articles = data.get(\"articles\", [])\n",
    "\n",
    "data is the dictionary you received from the NewsAPI response.\n",
    "\n",
    ".get(\"articles\", []) tries to extract the value under the key \"articles\"\n",
    "\n",
    " -> If \"articles\" exists → it returns the list of articles.\n",
    " -> If \"articles\" doesn’t exist → it returns an empty list [] instead of crashing. \n",
    "\n",
    "\n",
    "2.df = pd.DataFrame(articles)[[\"title\", \"description\", \"url\"]]\n",
    "\n",
    "Converts the articles list into a pandas DataFrame.\n",
    "Then selects only three columns:\n",
    "\t•\t\"title\"\n",
    "\t•\t\"description\"\n",
    "\t•\t\"url\"\n",
    "\n",
    "3.df.head()\n",
    "Shows the first 5 rows of the DataFrame.\n",
    "\n",
    "# ------------ Step:8 Run AI Agent for Analysis ---------------#\n",
    "\n",
    "results = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    title = row[\"title\"]\n",
    "    desc = row[\"description\"] or \"\"\n",
    "    text = f\"{title}. {desc}\"\n",
    "\n",
    "    try:\n",
    "        sentiment = sentiment_model(text[:512])[0]\n",
    "        summary = summarizer_model(text, max_length=40, min_length=10, do_sample=False)[0][\"summary_text\"]\n",
    "        entities = ner_model(text[:512])\n",
    "\n",
    "        results.append({\n",
    "            \"Title\": title,\n",
    "            \"Sentiment\": sentiment[\"label\"],\n",
    "            \"Score\": round(sentiment[\"score\"], 2),\n",
    "            \"Summary\": summary,\n",
    "            \"Entities\": [e[\"word\"] for e in entities],\n",
    "            \"URL\": row[\"url\"]\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "#------------------------Notes--------------------------#\n",
    "(a) results = []\n",
    "\n",
    "Creates an empty list that will store the final processed output for each news article\n",
    "\n",
    "(b) for idx, row in df.iterrows():\n",
    "    title = row[\"title\"]\n",
    "    desc = row[\"description\"] or \"\"\n",
    "    text = f\"{title}. {desc}\"\n",
    "\n",
    "Loops through each row of your DataFrame.\n",
    "\t•\tidx → row index (0,1,2…)\n",
    "\t•\trow → one full news article (title, description, url)\n",
    "\t•\trow[\"title\"] → fetches the news title\n",
    "\t•\trow[\"description\"] or \"\" → if description is missing, use empty string\n",
    "\t•\ttext → combines title + description into one text block\n",
    "\n",
    "[This text is fed into the models]\n",
    "\n",
    "(c) try:\n",
    "        sentiment = sentiment_model(text[:512])[0]\n",
    "\n",
    "Takes first 512 characters (text[:512])\n",
    "HuggingFace models have token limits\n",
    "Sends text to sentiment_model\n",
    "       \n",
    "\t(d)\tsummary = summarizer_model(text, max_length=40, min_length=10, do_sample=False)[0][\"summary_text\"]\n",
    "\n",
    "Feeds the full text to summarizer_model\n",
    "\t•\tmax_length=40 → summary will be short\n",
    "\t•\tmin_length=10 → minimum summary length\n",
    "\t•\tdo_sample=False → deterministic (no randomness)\n",
    "\t•\tExtracts only summary_text field.\n",
    "\n",
    "     (e) entities = ner_model(text[:512])\n",
    "\t\t\n",
    "\tSends the first 512 characters to the NER model\n",
    "\t•\tExtracts entities like:\n",
    "\t•\tPERSON\n",
    "\t•\tORG\n",
    "\t•\tLOCATION\n",
    "\t•\tDATE\n",
    "\t•\tEVENT\n",
    "\t•\tReturns a list of detected entities.\n",
    "\n",
    "     (f)   results.append({    (\n",
    "            \"Title\": title,\n",
    "            \"Sentiment\": sentiment[\"label\"],\n",
    "            \"Score\": round(sentiment[\"score\"], 2),\n",
    "            \"Summary\": summary,\n",
    "            \"Entities\": [e[\"word\"] for e in entities],\n",
    "            \"URL\": row[\"url\"]\n",
    "        })\n",
    "\t\t\n",
    "#-----------Notes:--------#---------#\n",
    "   Creates a dictionary for each article\n",
    "\t•\tExtracts:\n",
    "\t•\tSentiment label → \"POSITIVE\"\n",
    "\t•\tSentiment score → rounded to 2 decimals\n",
    "\t•\tSummary → BART summary\n",
    "\t•\tEntities → list of words only\n",
    "\t•\tAppends to results list\n",
    "\n",
    "      (g) except Exception as e:\n",
    "          print(f\"Error: {e}\")\n",
    "\n",
    "#----Notes:-------#\n",
    "If anything fails (API missing fields, model error), the script won’t stop — it prints the error and continues to the next article.\n",
    "\n",
    "#------------------------Step:9 Convert to Dataframe ------------------------#\n",
    "analyzed_df = pd.DataFrame(results)\n",
    "analyzed_df\n",
    "\n",
    "What this line does:\n",
    "\t•\tIt converts your results list (a list of dictionaries) into a pandas DataFrame.\n",
    "\t•\tEach dictionary in results becomes one row in the DataFrame.\n",
    "\t•\tEach key in the dictionary becomes a column name.\n",
    "\n",
    "So the DataFrame will have columns:\n",
    "\n",
    "Title\n",
    "Sentiment\n",
    "Score\n",
    "Summary\n",
    "Entities\n",
    "URL\n",
    "\n",
    "#-----------------------Step:10 Export the results to Csv file for further analysis ----------------_#\n",
    "\n",
    "analyzed_df.to_csv(\"news_analysis1_results.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
