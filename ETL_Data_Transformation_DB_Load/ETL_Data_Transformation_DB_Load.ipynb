{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ETL Data Transformation & DB Load\n",
                "\n",
                "## Project Overview\n",
                "This project demonstrates a classic **ETL (Extract, Transform, Load)** pipeline using the famous **Titanic dataset**. \n",
                "\n",
                "**Workflow:**\n",
                "1.  **Extract**: Load raw data (simulating a file read).\n",
                "2.  **Transform**: Clean data, handle missing values, and perform aggregations using **Pandas**.\n",
                "3.  **Load**: Store the processed data into a **PostgreSQL** database.\n",
                "4.  **Analyze**: Verify the load and perform SQL-based analysis.\n",
                "\n",
                "## Tech Stack\n",
                "- **Data Processing**: Pandas\n",
                "- **Database**: PostgreSQL\n",
                "- **ORM**: SQLAlchemy\n",
                "- **Visualization**: Seaborn (for loading the dataset)\n",
                "\n",
                "---\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install required packages (run once)\n",
                "!pip install -q pandas sqlalchemy psycopg2-binary seaborn\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Configuration\n",
                "Load database credentials securely.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import pandas as pd\n",
                "import seaborn as sns\n",
                "from sqlalchemy import create_engine\n",
                "\n",
                "# Database Configuration\n",
                "DB_HOST = os.getenv(\"DB_HOST\", \"localhost\")\n",
                "DB_PORT = os.getenv(\"DB_PORT\", \"5432\")\n",
                "DB_NAME = os.getenv(\"DB_NAME\", \"postgres\")\n",
                "DB_USER = os.getenv(\"DB_USER\", \"postgres\")\n",
                "DB_PASS = os.getenv(\"DB_PASS\", \"password\")\n",
                "\n",
                "# Create DB Connection\n",
                "connection_uri = f\"postgresql+psycopg2://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
                "engine = create_engine(connection_uri)\n",
                "\n",
                "print(\"Configuration loaded.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Extract (Load Data)\n",
                "We'll use the Titanic dataset available in Seaborn to simulate reading a raw file.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load raw data\n",
                "df_raw = sns.load_dataset('titanic')\n",
                "print(f\"Loaded {df_raw.shape[0]} rows and {df_raw.shape[1]} columns.\")\n",
                "df_raw.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Transform (Data Cleaning)\n",
                "Perform standard data cleaning tasks:\n",
                "- Handle missing values.\n",
                "- Normalize column names.\n",
                "- Create summary aggregations.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Normalize Columns (Strip whitespace, lowercase)\n",
                "df_raw.columns = df_raw.columns.str.strip().str.lower()\n",
                "\n",
                "# 2. Handle Missing Values\n",
                "# Fill numerical missing values with median\n",
                "num_cols = df_raw.select_dtypes(include=['int64', 'float64']).columns\n",
                "for col in num_cols:\n",
                "    median_val = df_raw[col].median()\n",
                "    df_raw[col] = df_raw[col].fillna(median_val)\n",
                "\n",
                "# Fill categorical missing values with 'unknown'\n",
                "cat_cols = df_raw.select_dtypes(include=['object', 'category']).columns\n",
                "for col in cat_cols:\n",
                "    df_raw[col] = df_raw[col].fillna(\"unknown\")\n",
                "\n",
                "print(\"Missing values handled.\")\n",
                "df_raw.isnull().sum()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Create Aggregations (Transformation)\n",
                "# Average fare by Sex\n",
                "fare_by_sex = df_raw.pivot_table(index='sex', values='fare', aggfunc='mean').reset_index()\n",
                "fare_by_sex.columns = ['sex', 'avg_fare']\n",
                "\n",
                "# Survival count by Class\n",
                "survival_by_class = df_raw.pivot_table(index='pclass', values='survived', aggfunc='sum').reset_index()\n",
                "survival_by_class.columns = ['pclass', 'survivor_count']\n",
                "\n",
                "print(\"Transformations complete.\")\n",
                "display(fare_by_sex)\n",
                "display(survival_by_class)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Load (Save to DB)\n",
                "Load the cleaned raw data and the aggregated insights into PostgreSQL.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "try:\n",
                "    # Load main cleaned table\n",
                "    df_raw.to_sql('titanic_cleaned', engine, if_exists='replace', index=False)\n",
                "    print(\"Loaded 'titanic_cleaned' table.\")\n",
                "    \n",
                "    # Load aggregated tables\n",
                "    fare_by_sex.to_sql('titanic_fare_by_sex', engine, if_exists='replace', index=False)\n",
                "    print(\"Loaded 'titanic_fare_by_sex' table.\")\n",
                "    \n",
                "    survival_by_class.to_sql('titanic_survival_by_class', engine, if_exists='replace', index=False)\n",
                "    print(\"Loaded 'titanic_survival_by_class' table.\")\n",
                "    \n",
                "except Exception as e:\n",
                "    print(f\"Error loading to DB: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Analysis (SQL Verification)\n",
                "Query the database to confirm the data is stored correctly.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Verify Data Load\n",
                "query = \"SELECT * FROM titanic_cleaned LIMIT 5\"\n",
                "df_verify = pd.read_sql(query, engine)\n",
                "df_verify"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Verify Aggregation Load\n",
                "query_agg = \"SELECT * FROM titanic_survival_by_class\"\n",
                "df_agg_verify = pd.read_sql(query_agg, engine)\n",
                "df_agg_verify"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Conclusion\n",
                "This pipeline successfully:\n",
                "1.  Ingested raw Titanic data.\n",
                "2.  Cleaned missing values and standardized columns.\n",
                "3.  Generated key insights (aggregations).\n",
                "4.  Persisted both the cleaned raw data and insights into a relational database.\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}