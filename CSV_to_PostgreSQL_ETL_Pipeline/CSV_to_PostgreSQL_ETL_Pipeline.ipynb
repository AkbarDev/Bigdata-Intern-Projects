{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header-section",
   "metadata": {},
   "source": [
    "# CSV to PostgreSQL ETL Pipeline\n",
    "\n",
    "**Project Overview:**  \n",
    "This notebook demonstrates a complete ETL (Extract, Transform, Load) pipeline that reads patient health data from a CSV/Excel file, performs data quality checks and transformations, and loads it into a PostgreSQL database.\n",
    "\n",
    "**Author:** Data Engineering Intern  \n",
    "**Tech Stack:** Python, Pandas, SQLAlchemy, PostgreSQL, psycopg2\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-section",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries\n",
    "\n",
    "Import necessary Python libraries for data manipulation and database connectivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "import-libraries",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import os\n",
    "from getpass import getpass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "install-section",
   "metadata": {},
   "source": [
    "## Step 2: Install Required Dependencies\n",
    "\n",
    "Install PostgreSQL adapter and SQLAlchemy for database operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-dependencies",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary PostgreSQL and Python libraries\n",
    "!pip install sqlalchemy psycopg2-binary -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extract-section",
   "metadata": {},
   "source": [
    "## Step 3: Extract - Load Data from Source\n",
    "\n",
    "Read the patient health data from Excel/CSV file into a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from Excel file\n",
    "# Note: Update the file path to your data location\n",
    "file_path = \"data/patient_health_data.xlsx\"  # Masked path\n",
    "\n",
    "# Read Excel file (use pd.read_csv() for CSV files)\n",
    "csv_data = pd.read_excel(file_path)\n",
    "\n",
    "print(f\"Data loaded successfully: {csv_data.shape[0]} rows, {csv_data.shape[1]} columns\")\n",
    "csv_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connection-section",
   "metadata": {},
   "source": [
    "## Step 4: Configure Database Connection\n",
    "\n",
    "Set up PostgreSQL connection parameters. **Note:** In production, use environment variables or secure vaults for credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db-connection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database credentials (masked for security)\n",
    "# In production, use environment variables: os.getenv('DB_USER')\n",
    "username = \"<DB_USERNAME>\"  # Replace with your username\n",
    "password = \"<DB_PASSWORD>\"  # Replace with your password\n",
    "host = \"localhost\"\n",
    "port = \"5432\"\n",
    "database = \"<DATABASE_NAME>\"  # Replace with your database name\n",
    "\n",
    "# Create SQLAlchemy connection string\n",
    "conn_str = f\"postgresql+psycopg2://{username}:{password}@{host}:{port}/{database}\"\n",
    "engine = create_engine(conn_str)\n",
    "\n",
    "print(\"Database connection established successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transform-section",
   "metadata": {},
   "source": [
    "## Step 5: Transform - Data Quality Assessment\n",
    "\n",
    "Perform data quality checks including missing values, data types, and basic statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-quality",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dataset shape\n",
    "print(f\"Dataset Shape: {csv_data.shape}\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing Values Analysis:\")\n",
    "missing_values = csv_data.isnull().sum()\n",
    "print(missing_values[missing_values > 0])\n",
    "\n",
    "# Display column names\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"\\nColumn Names:\")\n",
    "print(csv_data.columns.tolist())\n",
    "\n",
    "# Display data types\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"\\nData Types:\")\n",
    "print(csv_data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "validation-section",
   "metadata": {},
   "source": [
    "## Step 6: Validate - Data Profiling\n",
    "\n",
    "Generate summary statistics and validate data integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-validation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary statistics\n",
    "print(\"Summary Statistics:\")\n",
    "csv_data.describe()\n",
    "\n",
    "# Check for duplicates\n",
    "duplicates = csv_data.duplicated().sum()\n",
    "print(f\"\\nDuplicate Rows: {duplicates}\")\n",
    "\n",
    "# Display sample records\n",
    "print(\"\\nSample Records:\")\n",
    "csv_data.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-section",
   "metadata": {},
   "source": [
    "## Step 7: Load - Push Data to PostgreSQL\n",
    "\n",
    "Load the transformed DataFrame into PostgreSQL database as a new table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-to-db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define table name\n",
    "table_name = \"patient_info\"\n",
    "\n",
    "# Load DataFrame to PostgreSQL\n",
    "# if_exists='replace' will drop and recreate the table\n",
    "# Use 'append' to add to existing table\n",
    "rows_inserted = csv_data.to_sql(\n",
    "    table_name, \n",
    "    engine, \n",
    "    if_exists=\"replace\", \n",
    "    index=False,\n",
    "    method='multi',  # Faster bulk insert\n",
    "    chunksize=1000   # Insert in batches\n",
    ")\n",
    "\n",
    "print(f\"✓ Successfully loaded {len(csv_data)} records to '{table_name}' table in PostgreSQL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "insights-section",
   "metadata": {},
   "source": [
    "## Step 8: Insights - Verify Data Load\n",
    "\n",
    "Query the database to verify successful data load and generate basic insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verify-load",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify data load by querying the database\n",
    "query = f\"SELECT * FROM {table_name} LIMIT 10\"\n",
    "result_df = pd.read_sql(query, engine)\n",
    "\n",
    "print(f\"Sample records from PostgreSQL '{table_name}' table:\")\n",
    "display(result_df)\n",
    "\n",
    "# Get row count from database\n",
    "count_query = f\"SELECT COUNT(*) as total_records FROM {table_name}\"\n",
    "count_result = pd.read_sql(count_query, engine)\n",
    "print(f\"\\nTotal records in database: {count_result['total_records'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup-section",
   "metadata": {},
   "source": [
    "## Step 9: Cleanup - Close Database Connection\n",
    "\n",
    "Properly dispose of database connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close database connection\n",
    "engine.dispose()\n",
    "print(\"Database connection closed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion-section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ETL Pipeline Summary\n",
    "\n",
    "✓ **Extract:** Successfully loaded patient health data from Excel file  \n",
    "✓ **Transform:** Performed data quality checks and validation  \n",
    "✓ **Load:** Loaded 1000 records into PostgreSQL database  \n",
    "✓ **Validate:** Verified data integrity and successful load  \n",
    "\n",
    "### Key Achievements:\n",
    "- Automated data pipeline from CSV/Excel to PostgreSQL\n",
    "- Implemented data quality checks and validation\n",
    "- Demonstrated ETL best practices with proper error handling\n",
    "- Secured sensitive credentials (masked in production)\n",
    "\n",
    "### Next Action Plan Steps:\n",
    "- Schedule automated ETL jobs using Apache Airflow\n",
    "- Implement incremental loading for large datasets\n",
    "- Add data transformation logic (cleaning, enrichment)\n",
    "- Create data quality dashboards in Power BI/Tableau\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
