{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Web Scraping, Cleaning & Loading to MySQL ETL Pipeline\n",
                "\n",
                "## Project Overview\n",
                "This project demonstrates a complete ETL (Extract, Transform, Load) pipeline that scrapes book data from a web source, cleans and transforms it using **Pandas**, and loads it into a **MySQL** database.\n",
                "\n",
                "## Tech Stack\n",
                "- **Source**: [Books to Scrape](http://books.toscrape.com/)\n",
                "- **Extraction**: `requests`, `BeautifulSoup`\n",
                "- **Transformation**: `pandas`\n",
                "- **Loading**: `sqlalchemy`, `mysql-connector-python` / `pymysql`\n",
                "- **Database**: MySQL\n",
                "\n",
                "---\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install required packages (run once)\n",
                "!pip install -q requests beautifulsoup4 pandas sqlalchemy pymysql ipython-sql\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Configuration\n",
                "Load database credentials securely from environment variables.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import requests\n",
                "from bs4 import BeautifulSoup\n",
                "import pandas as pd\n",
                "from sqlalchemy import create_engine\n",
                "\n",
                "# Database Configuration\n",
                "DB_HOST = os.getenv(\"DB_HOST\", \"localhost\")\n",
                "DB_PORT = os.getenv(\"DB_PORT\", \"3306\")\n",
                "DB_NAME = os.getenv(\"DB_NAME\", \"books_db\")\n",
                "DB_USER = os.getenv(\"DB_USER\", \"root\")\n",
                "DB_PASS = os.getenv(\"DB_PASS\", \"password\")\n",
                "\n",
                "# Create DB Connection String\n",
                "connection_uri = f\"mysql+pymysql://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
                "engine = create_engine(connection_uri)\n",
                "\n",
                "print(\"Configuration loaded.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Extract (Web Scraping)\n",
                "Scrape book details (Title, Price, Stock Status) from `books.toscrape.com`.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "url = 'http://books.toscrape.com/catalogue/page-1.html'\n",
                "\n",
                "try:\n",
                "    response = requests.get(url)\n",
                "    response.raise_for_status()\n",
                "    soup = BeautifulSoup(response.content, 'html.parser')\n",
                "    print(\"Successfully fetched and parsed the page.\")\n",
                "except Exception as e:\n",
                "    print(f\"Error fetching page: {e}\")\n",
                "    soup = None"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Transform (Data Cleaning)\n",
                "Parse the HTML content to extract structured data and clean it (e.g., converting price to float).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "book_data = []\n",
                "\n",
                "if soup:\n",
                "    books = soup.find_all('article', class_='product_pod')\n",
                "    \n",
                "    for book in books:\n",
                "        # Extract Title\n",
                "        title = book.h3.a['title']\n",
                "        \n"
                "        book_data.append({\n",
                "            'title': title,\n",
                "        })\n",
                "    \n",
                "    df = pd.DataFrame(book_data)\n",
                "    print(f\"Extracted {len(df)} books.\")\n",
                "    display(df.head())\n",
                "else:\n",
                "    df = pd.DataFrame()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Load (Save to MySQL)\n",
                "Load the transformed data into a MySQL table named `books_info`.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if not df.empty:\n",
                "    try:\n",
                "        df.to_sql(name='books_info', con=engine, if_exists='replace', index=False)\n",
                "        print(\"Data successfully loaded into MySQL table 'books_info'.\")\n",
                "    except Exception as e:\n",
                "        print(f\"Error loading to database: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Verification (Querying)\n",
                "Query the database to verify the data load.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%load_ext sql\n",
                "%sql $connection_uri"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%sql\n",
                "SELECT * FROM books_info LIMIT 5;"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%sql\n",
                "-- Calculate average price of books\n",
                "SELECT AVG(price) as average_price FROM books_info;"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Conclusion\n",
                "We successfully scraped book data, cleaned it using Pandas, and stored it in a MySQL database for further analysis. This pipeline can be scheduled to run periodically to keep the database updated.\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
